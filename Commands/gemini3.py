from datetime import datetime
from urllib.parse import urlencode, unquote, urlparse
from itertools import zip_longest
import re, html2text
from google import genai
from google.genai import types
from Utils.utils import (
    proxy_request,
    clean_str,
    send_chunks,
    fetch_cmd_data,
    gemini_generate,
    check_cooldown,
    parse_str
)

MODEL_NAME = "gemini-flash-lite-latest"
GENERATION_CONFIG = {
    "max_output_tokens": 400,
    "temperature": 0.3,
    "top_p": 0.95,
    "system_instruction": [
        types.Part.from_text(text="Please provide a short, concise response with enough detail. Do not ask the user follow up questions, because you are intended to provide a single response with no history and are not expected any follow up prompts. Answer should be at most 990 characters.")
    ]
}

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)",
    "Accept": "text/html,application/xhtml+xml",
    "Accept-Language": "en-US,en;q=0.9",
    "Referer": "https://example.com",
}

def fetch_and_parse_html(url):
    try:
        res = proxy_request("GET", url, headers=headers)
        if not res or not res.text:
            print(f"fetch_and_parse_html: Empty response from {url}")
            return None

        return parse_str(res.text, "html")

    except Exception as e:
        print(f"fetch_and_parse_html: Error fetching/parsing {url}: {e}")
        return None
 
def get_duckduckgo_results(query):
    url = "https://lite.duckduckgo.com/lite/?" + urlencode({"q": query})

    soup = fetch_and_parse_html(url)
    if not soup:
        return []

    a_elements = soup.select('.result-link')
    urls = []

    for a in a_elements:
        try:
            decoded_href = unquote(a['href'])
            match = re.search(r"http.*?(?=&)", decoded_href)
            url = match.group(0) if match else None
            if not url or "ad_domain=" in url:
                continue
            urls.append(url)
        except Exception as e:
            print(f"[Error] {e}")

    return urls

def get_google_lucky(query):
    params = {'q': query, 'btnI': "I'm Feeling Lucky"}
    query_string = urlencode(params)
    url = f"https://www.google.com/search?{query_string}"

    try:
        res = proxy_request("GET", url, headers=headers)
        if res is None:
            print("get_google_lucky: No response object")
            return None, "No response"

        if res.status_code not in [200, 301, 302]:
            print(f"get_google_lucky: Unexpected status {res.status_code} from {url}")
            print(f"Response snippet: {res.text[:200] if res.text else 'Empty'}")
            return None, res.status_code

        # Check for transparent redirect
        final_url = getattr(res, 'url', '')
        if final_url and "google.com" not in final_url:
            print(f"get_google_lucky: Redirected to {final_url}")
            return final_url, None

        # Check Location header (handles /url?q= redirect format)
        location = res.headers.get("Location", "")
        if location:
            match = re.search(r'[?&]q=(https?://[^&]+)', location)
            if match:
                destination = unquote(match.group(1))
                if "google.com" not in destination:
                    print(f"get_google_lucky: Extracted URL from Location param: {destination}")
                    return destination, None
            if "google.com" not in location and location.startswith('http'):
                print(f"get_google_lucky: Got redirect to {location}")
                return location, None

        # Parse HTML body for Redirect Notice links
        body = res.text
        if body:
            soup = parse_str(body, "html")
            if soup:
                for link in soup.find_all('a', href=True):
                    href = link['href']
                    # Look for external URLs or Google-wrapped links
                    if href.startswith('http') and 'google.com' not in href:
                        print(f"get_google_lucky: Found link in HTML: {href}")
                        return href, None
                    if '/url?' in href or 'google.com/url?' in href:
                        match = re.search(r'[?&]q=(https?://[^&]+)', href)
                        if match:
                            destination = unquote(match.group(1))
                            if "google.com" not in destination:
                                print(f"get_google_lucky: Extracted URL from link param: {destination}")
                                return destination, None

        # Direct fallback (bypass proxy) to capture raw redirect
        try:
            res_direct = proxy_request("GET", url, headers=headers, bypass_proxy=True, allow_redirects=False, timeout=5)
            if res_direct:
                loc = res_direct.headers.get("Location", "")
                if loc:
                    match = re.search(r'[?&]q=(https?://[^&]+)', loc)
                    if match:
                        destination = unquote(match.group(1))
                        if "google.com" not in destination:
                            print(f"get_google_lucky: Direct fallback extracted URL: {destination}")
                            return destination, None
                    if "google.com" not in loc and loc.startswith('http'):
                        print(f"get_google_lucky: Direct fallback redirect to {loc}")
                        return loc, None
        except Exception as e:
            print(f"get_google_lucky: Direct fallback failed: {e}")

        print("get_google_lucky: Could not extract destination URL")
        return None, None

    except Exception as e:
        print(f"get_google_lucky: Error: {e}")
        return None, "Error"

def get_wikipedia_snippet(query):
    try:
        ddg_results = get_duckduckgo_results(query + " site:wikipedia.org")

        if not ddg_results or not isinstance(ddg_results, list):
            return

        path = urlparse(ddg_results[0]).path
        if not path.startswith('/wiki/'):
            return
        title = path.split('/wiki/')[-1]

        url = "https://en.wikipedia.org/w/api.php"
        params = {
            "action": "parse",
            "page": title,
            "format": "json",
            "origin": "*"
        }
        res = proxy_request("GET", url, params=params)
        if not res or res.status_code != 200:
            return

        json_data = res.json()
        text = json_data.get("parse", {}).get("text", {}).get("*", "")
        soup = parse_str(text, "html")
        if soup is None:
            return

        content = []
        for elem in soup.find_all(['p', 'h2']):
            if elem.name == 'h2':
                break
            if elem.name == 'p':
                content.append(str(elem))

        intro_html = "".join(content)

        text_maker = html2text.HTML2Text()
        text_maker.ignore_links = True
        text_maker.ignore_emphasis = True
        text_maker.ignore_images = True
        text_maker.ignore_tables = True
        text_maker.body_width = 0

        snippet = text_maker.handle(intro_html).strip()

        return snippet

    except Exception as e:
        print(f"[Error] {e}")
        return

def querify(prompt):
    model_name = "gemma-3n-e4b-it"
    config = { "max_output_tokens": 400, "temperature": 0.3 }
    prompt = f"Convert into a keyword Google search query if needed. Return only the resulting query, without quotes or any extra text: {prompt}"
    result = gemini_generate(prompt, model_name, config)
    if not result:
        return
    
    return result

def get_body_content(url):
    soup = fetch_and_parse_html(url)
    if not soup:
        return ""

    if not soup.body:
        print("get_body_content: <body> tag not found.")
        return ""

    return soup.body.get_text(" ", strip=True)

def get_grounding_data(prompt, count=2):
    blocked_domains = {'facebook.com', 'youtube.com', 'reddit.com', 'instagram.com'}
    modifiers = ' ' + ' '.join(f'-site:{domain}' for domain in blocked_domains)

    normal_urls = get_duckduckgo_results(prompt + modifiers)
    if normal_urls:
        print("Normal URLs:", normal_urls)

    query_urls = []
    query = querify(prompt)

    if query and query.strip() != prompt:
        print("Query:", query)
        query_urls = get_duckduckgo_results(query + modifiers)
        if query_urls:
            print("Query URLs:", query_urls)

    google_lucky_url, error_code = get_google_lucky(prompt)
    google_urls = [google_lucky_url] if google_lucky_url else []
    if google_lucky_url:
        print("Google URL:", google_lucky_url)

    if not (normal_urls or query_urls or google_urls) and not error_code:
        return

    valid_urls = []
    contents = []

    seen = set()
    for group_idx, group in enumerate(zip_longest(normal_urls, query_urls, google_urls)):
        print(f"Group {group_idx}: {group}")
        for url_idx, url in enumerate(group):
            print(f"  URL {url_idx}: {url}")
            if not url:
                print("    Skipped: URL is None or empty")
                continue
            if url in seen:
                print("    Skipped: URL already seen")
                continue
            domain = urlparse(url).netloc
            print(f"    Domain: {domain}")
            if any(blocked_domain in url for blocked_domain in blocked_domains):
                print("    Skipped: Domain blocked")
                continue
            content = get_body_content(url)
            if not content:
                print("    Skipped: No content retrieved")
                continue
            seen.add(url)
            valid_urls.append(url)
            contents.append(content)
            print(f"    Added URL, total valid URLs: {len(valid_urls)}")
            if len(valid_urls) == count:
                print("    Reached count limit, breaking")
                break
        if len(valid_urls) == count:
            break

    wikipedia_snippet = get_wikipedia_snippet(prompt)
    if wikipedia_snippet:
        print(f"Wikipedia Snippet: {wikipedia_snippet[:300]}\n")
    combined_content = "\n".join(contents)
    return {
        'body_content': combined_content,
        'wikipedia_snippet': wikipedia_snippet,
        'valid_urls': valid_urls,
        'error_code': error_code
    }

def reply_with_grounded_gemini(self, message):
    cmd = fetch_cmd_data(self, message)
    
    if not check_cooldown(cmd.state, cmd.nick, cmd.cooldown):
        return
    
    if not cmd.params:
        m = (
            f"{cmd.username}, please provide a prompt for Gemini. "
            f"Model: {MODEL_NAME}, temperature: {GENERATION_CONFIG['temperature']}, "
            f"top_p: {GENERATION_CONFIG['top_p']}"
        )
        self.send_privmsg(cmd.channel, m)
        return

    prompt = cmd.params.strip()
    try:
        utc_date_time = datetime.now().strftime("%A %d %B %Y %I:%M %p UTC")
        
        grounding_data = get_grounding_data(prompt)

        if not grounding_data:
            self.send_privmsg(cmd.channel, "No results found for the query.")
            return

        error_code = grounding_data.get('error_code')
        valid_urls = grounding_data['valid_urls']

        wikipedia_snippet = grounding_data.get('wikipedia_snippet')

        if not valid_urls and not wikipedia_snippet and error_code:
            if error_code == "No response":
                self.send_privmsg(cmd.channel, f"{cmd.username}, search failed, try again later.")
            else:
                self.send_privmsg(cmd.channel, f"Error: Request failed with code {error_code}.")
            return

        if not valid_urls and not wikipedia_snippet:
            self.send_privmsg(cmd.channel, "No results found for the query.")
            return
        grounding_text = (
            f"Today is {utc_date_time}.\n\n"
            "Use what's relevant of this text to inform your response to the prompt above (Don't mention that I provided you with a text/document/article/context for your response under any circumstance. Answer as if you know this information):\n"
            f"{wikipedia_snippet or ''}\n"
            f"{grounding_data.get('body_content', '')}"
        )

        is_grounded = bool(valid_urls) or bool(wikipedia_snippet)

        result = gemini_generate({
            "prompt": prompt,
            "grounded": is_grounded,
            "grounding_text": grounding_text
        }, MODEL_NAME, GENERATION_CONFIG)

        if not result:
            self.send_privmsg(cmd.channel, "Failed to generate a response. Please try again later.")
            return
        
        clean_result = clean_str(result, ['`', '*'])
        send_chunks(self.send_privmsg, cmd.channel, clean_result)
        if valid_urls:
            self.send_privmsg(cmd.channel, f"üìù Source(s): {' | '.join(valid_urls)}")

    except Exception as e:
        print(f"[Error] {e}")
        self.send_privmsg(cmd.channel, "Failed to send a response. Please try again later.")
        return